{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image as PILImage\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, Image\n",
    "\n",
    "from transformers import ViTImageProcessor\n",
    "from transformers import ViTForImageClassification\n",
    "from transformers import ViTFeatureExtractor\n",
    "from transformers import ViTModel\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model2 = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTLayer(\n",
       "  (attention): ViTSdpaAttention(\n",
       "    (attention): ViTSdpaSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (output): ViTSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): ViTIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): ViTOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model1.eval()\n",
    "model1.pooler.train()\n",
    "model1.encoder.layer[0].train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\", add_pooling_layer=False)\n",
    "model2.eval()\n",
    "model2.encoder.layer[0].train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model2.eval()\n",
    "# model2.encoder.layer[0].train()\n",
    "\n",
    "\n",
    "# y1 = model1(x) ---> y1[1] returns head_outputs + encoder_outputs[1:]\n",
    "# y2 = model2(x) \n",
    "\n",
    "# y3 = MLP(y1[0]) instancia de clasificación\n",
    "# y4 = MLP(y2[0]) instancia de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfae113d60f744368ccb504f4308dccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/168 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "images_path = Path(r'C:\\Users\\lzuni\\Documents\\Tesis\\database\\patches')\n",
    "labels_path = Path(r'C:\\Users\\lzuni\\Documents\\Tesis\\database\\patch_labels.csv')\n",
    "\n",
    "\n",
    "images = sorted(list(images_path.glob('*.tiff')), key=lambda x: int(x.stem.split('patch')[-1]))\n",
    "labels = pd.read_csv(labels_path)\n",
    "dataset_dict = {'image': [], 'label': []}\n",
    "\n",
    "for image, label in zip(images, labels['labels']):\n",
    "    dataset_dict['image'].append(PILImage.open(image))\n",
    "    label2name = {\"1\": \"NT\", \"2\": \"CLE\", \"3\": \"PSE\"}\n",
    "    dataset_dict['label'].append(label2name[str(label)])\n",
    "\n",
    "ds = Dataset.from_dict(dataset_dict )\n",
    "ds = ds.class_encode_column('label')\n",
    "\n",
    "splits = ds.train_test_split(test_size=0.2, stratify_by_column='label')\n",
    "train_ds = splits['train']\n",
    "test_ds = splits['test']\n",
    "splits2 = train_ds.train_test_split(test_size=0.2, stratify_by_column='label')\n",
    "train_ds = splits2['train']\n",
    "val_ds = splits2['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract label counts for each dataset\n",
    "train_labels = [int(label) for label in train_ds['label']]\n",
    "test_labels = [int(label) for label in test_ds['label']]\n",
    "val_labels = [int(label) for label in val_ds['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['label'] = example_batch['label']\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "    labels = torch.tensor([example[\"label\"] for example in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzuni\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "model_name = 'google/vit-base-patch32-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "image_mean, image_std = feature_extractor.image_mean, feature_extractor.image_std\n",
    "size = feature_extractor.size[\"height\"]\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            # normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            # normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)\n",
    "test_ds.set_transform(val_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAD0AAAA9EAAAAABrmeJ2AAABR0lEQVR4Ae3SoUsEQRQG8O97u+cVFQWDqFgUFoNBk4YNV85sEawGqwiCwXDB/8CkGE0GwWAxKILgyonKFYPnoSgICgaDnsHdsR93sBxzs4a3bYc37833mwH0UwEVUAEVUAEVUAEVUAEVUAEVUAEVUAEVcCkwjbF2xkk7mxr2DOCpYUV//6FAmN2Zrps9sz30dPxEI6x0fEaLAZMSNUvdotrq8oVEVvulbtbFtVw3U5fbLDySnISZgHNb+s2KzSxpe63yxX+TmSzAz71vDsenGYBzi4E5wW1aJHt1JX76DxJg133qKymiZqq4dH7XLHjj5iNewLtvTzJVpyLL2DdzeMSr69Q/3iYrcZ9ZQt3xXbPEgsmbEFWcuU29IfPym/QmN5jCutvUAeucxQ483GHZ6TPjAUeTyJRxD8Gi09R8xiG+cIxBTGDoD+WoQIYjmyd7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.TiffImagePlugin.TiffImageFile image mode=I size=61x61>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[7]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch32-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "id2label = {\"0\": \"NT\", \"1\": \"CLE\", \"2\": \"PSE\"}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "config = AutoConfig.from_pretrained(model_name, label2id=label2id, id2label=id2label)\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([4, 3, 224, 224])\n",
      "labels torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=collate_fn, batch_size=4)\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-base-patches\",\n",
    "  use_cpu=True,\n",
    "  per_device_train_batch_size=4,\n",
    "  per_device_eval_batch_size= 1,\n",
    "  eval_strategy=\"epoch\",\n",
    "  save_strategy=\"epoch\",\n",
    "  num_train_epochs=4,\n",
    "  # weight_decay=0.01,\n",
    "  fp16=True,\n",
    "  # save_steps=100,\n",
    "  # eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=1,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108/108 01:35, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.979200</td>\n",
       "      <td>0.770187</td>\n",
       "      <td>0.592593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.919100</td>\n",
       "      <td>0.909654</td>\n",
       "      <td>0.481481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.925900</td>\n",
       "      <td>0.703723</td>\n",
       "      <td>0.629630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.892200</td>\n",
       "      <td>0.700396</td>\n",
       "      <td>0.629630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=108, training_loss=0.8923925073058517, metrics={'train_runtime': 97.8998, 'train_samples_per_second': 4.372, 'train_steps_per_second': 1.103, 'total_flos': 3.3807228049760256e+16, 'train_loss': 0.8923925073058517, 'epoch': 4.0})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "# train_results = trainer.train()\n",
    "# # trainer.save_model()\n",
    "# trainer.log_metrics(\"train\", train_results.metrics)\n",
    "# trainer.save_metrics(\"train\", train_results.metrics)\n",
    "# trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.6296\n",
      "  eval_loss               =     0.7004\n",
      "  eval_runtime            = 0:00:03.27\n",
      "  eval_samples_per_second =       8.24\n",
      "  eval_steps_per_second   =       8.24\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(val_ds)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x261d3394470>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "outputs = trainer.predict(test_ds)\n",
    "\n",
    "y_true = outputs.label_ids\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "\n",
    "labels = test_ds.features['label'].names\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
